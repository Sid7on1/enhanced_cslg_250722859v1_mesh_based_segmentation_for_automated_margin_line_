{
  "agent_id": "coder4",
  "task_id": "task_6",
  "files": [
    {
      "name": "augmentation.py",
      "purpose": "Data augmentation techniques",
      "priority": "medium"
    },
    {
      "name": "feature_extraction.py",
      "purpose": "Feature extraction layers",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.LG_2507.22859v1_Mesh_based_segmentation_for_automated_margin_line_",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.LG_2507.22859v1_Mesh-based-segmentation-for-automated-margin-line- with content analysis. Detected project type: computer vision (confidence score: 8 matches).",
    "key_algorithms": [
      "Constrained",
      "Splprep",
      "Predicted",
      "Other",
      "Semantic",
      "Line",
      "Extracting",
      "Cut",
      "Extraction",
      "Edge"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.LG_2507.22859v1_Mesh-based-segmentation-for-automated-margin-line-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nPre-print  \n \n Mesh -based segmentation for automated margin line \ngeneration  on incisors  receiving  crown treatment  \nAmmar Alsheghri 1,5 *, Ying Zhang 2, Farnoosh Ghadiri 3, Julia Keren 4, Farida Cheriet 2 and Francois \nGuibault 2 \n1 Mechanical Engineering Department, King Fa hd University of Petroleum and Minerals (KFUPM), Dhahran, 31261, KSA  \n2 Department of Computer and Software Engineering, Polytechnique Montr\u00e9al, Montr\u00e9al, QC, Canada  \n3 Centre d'intelligence artificielle appliqu\u00e9e (JACOBB), Montr\u00e9al, QC, Canada  \n4 Intellident  Dentaire Inc., Montr\u00e9al, QC, Canada  \n5 Interdisciplinary research center for Biosystems  and Machines , King Fahd University of Petroleum and Minerals (KFUPM), Dhahran, \n31261 , KSA  \n \n* Correspondence: ammar.sheghri@kfupm.edu.sa  \nAbstract: Dental crowns are esse ntial dental treatments for restoring damaged or missing teeth of patients. Recent design \napproaches of dental crowns are carried out using commercial dental design software. Once a scan of a preparation is \nuploaded to the software, a dental technician nee ds to manually define a precise margin line on the preparation surface \nwhich constitutes a nonrepeatable and inconsistent procedure.  This work proposes a new framework to determine margin \nlines automatically and accurately using deep learning. A dataset of  incisor teeth was provided by a collaborating dental \nlaboratory to train a deep learning segmentation model. A mesh -based  neural  network was modified by changing its input \nchannels and used to segment the prepared tooth in two regions such that the margin  line is contained within the boundary \nfaces separating the two regions. Next, k -fold cross -validation was used to train 5 models and a voting classifier technique \nwas used to combine their results to enhance the segmentation. After that, boundary smoothen ing and optimization using \nthe graph cut method was applied to refine the segmentation results. Then , boundary faces separating the two regions were \nselected to represent the margin line faces. A spline was approximated to best fit the centers of the bound ary faces to predict \nthe margin line. Our results show that an ensemble mo del combined with maximum probability predicted the highest \nnumber of  successful test cases (7 out of 13) based on a  maximum  distance threshold of 200 \u03bcm  (representing human error) \nbetween the predicted and ground truth point clouds. It was also demonstrated that the better the quality of the preparation, \nthe smaller the divergence between the predicted and ground truth margin lines (Spearman\u2019s rank correlation coefficient of \n-0.683).  We provide the train and test datasets for the community1. \nKeywords: Tooth restoration design; Mesh -based segmentation ; 3D Deep learning; Digital dentistry, Dental preparation  \nmargin line . Artificial intelligence; Digital systems  \n \n1. Introduction  \nThe extr action of a margin line from a dental preparation is the first and most important step in the crown \ngeneration process. A dental crown is a treatment used to restore a patient\u2019s damaged tooth and it is considered \namong the most essential components of dent al restorations. To receive a crown treatment, a dental preparation \nis created from the damaged tooth by removing the diseased tooth sections and molding the patient\u2019s tooth. \nFigure 1 shows a typical crown restoration procedure where a designed crown is pl aced on a dental preparation. \nThe dental crown sits on the prepared tooth and seals it at the finish line or the margin line as shown in Figure \n1. The margin line is defined as the terminal/peripheral portion of the tooth preparation [1]. This structure is  \nessential for adequate seating of the crown [2].  \n                                                           \n1 https://github.com/intellident -ai/teethPreparationData/tree/main . \n\n--- Page 2 ---\nPre-print  \n \nPage 2 of 15 \n  \nFigure 1. The accurate extraction of the margin line is essential for a successful crown generation. Accurate extraction of the \nmargin line is challenging in areas of a dental preparation that are flat su ch as the surface A (i.e., fuzzy region that has no \ngeometric features) whereas it is easier for surfaces of high curvature such as surface B.  \nWhile preparing a tooth to receive a dental crown, the dentist locates an appropriate margin line on the \ndamaged tooth such that undercuts are avoided. It is essential to prepare the tooth with an adequate margin \nline for the success of the dental crown treatment [3, 4]. In practice, dentists try to conserve the maximum \namount of tooth structure while preparing a too th to create enough space and ensure the placement of a \nretentive and esthetic crown. The preparation should help the retention by resisting the crown restoration from \nremoval along its path of insertion. Once a die is received by a dental technician to cr eate a crown, a commercial \ndental restoration software is used by the technician to manually select the points of the margin line on the die. \nThe margin line extraction process is a critical and difficult step in the crown restoration procedure. Despite \nadvances in digital dentistry, the manual selection process makes the procedure non -repeatable, variable, and \ntime -consuming depending on the experience of the dental technician [5]. Changes in the margin line design \ncan result in differences in marginal ada ptation of crowns [6] and can also affect the failure load and \nfractography of the crown [7]. It was reported that around 12.7% of dental crown restorations suffer from poor \nretention due to food residue coming from poor fitting at the gingival margin [8].   \nExtraction of margin lines from a 3D die geometry has been previously investigated in the literature [9]. \nNevertheless, the problem of accurately determining a margin line is difficult because the dental margin line \ndoes not necessarily always follow the  feature line as shown in Figure 1. There is very few published research \non methods of margin line extraction. In [10] a margin line extracting algorithm based on a heuristic searching \nstrategy based on feature information was presented. In [11], the A* al gorithm was used to search for the \nminimum cost path for a graph that represents the outermost feature curve. However, such techniques still \ndepend on the presence of a feature line region which could be missing in certain regions.  \nAI models are being incr easingly and successfully used in digital dentistry including treatment planning \nin prosthodontics, orthodontics, and esthetic dentistry procedures  [12 -13]. In [14] deep learning was used to \ngenerate a crown shell for a dental restoration using multi -resolution generative neural networks. Later, \ntransformers were used to replace neural networks in an attempt to improve the generation quality [15]. A \ndetailed end -to-end framework for dental crown design using AI with preprocessing and postprocessing \nprocedu res are presented in [16]. In [17] a deep adversarial network -driven gingival margin line reconstruction \nframework was introduced to automatically obtain the personalized gingival contour for a partially edentulous \npatient. This work achieved superior perf ormance compared to recent advances on real -world dental databases \nby preserving gingival contour details, structure, and perceptual features through a dual generator model and \ntwo-scale discriminator model, and reconstructing missing gingival margin lines  harmoniously with adjacent \nteeth. Nevertheless, margin line reconstruction on surfaces of prepared teeth for dental crown treatment was \nnot discussed. The features distinguishing margin lines between teeth and gingiva are different than the those \ndistingu ishing margin lines on surfaces of prepared teeth. Other studies used deep learning for teeth \nsegmentation and labeling [18 -20]. In the context of 3D surface data coming from optical scanners in the form \nof a triangular mesh, segmentation refers to disting uishing triangles of different classes or groups with different \nlabels. Regarding automated 3D teeth segmentation from intraoral scans and labeling, researchers have \n\n\n--- Page 3 ---\nPre-print  \n \nPage 3 of 15 \n achieved high efficiency using deep learning for applications in orthodontic diagnosis and  appliance fabrication \n[21]. On the other hand, only a single study focused on the use of deep learning for margin line extraction from \nsurfaces of prepared teeth with a dataset of 380 cases and reported enhanced results of automatically generating \na margi n line using AI compared with traditional methods [8, 22]. The S -Octree technique was used to divide \nthe dental preparation into two parts along the feature line representing the margin line and providing ground \ntruth labels to train a segmentation neural network. However, this might not be suited to provide ground truth \ndata especially for dies with an abundance of fuzzy regions that have missing feature lines as shown, for \nexample, by surface \u2018A\u2019 in Figure 1.  \n \nIn practice, a dental crown is hollowed from  inside and it consists of two main parts: a crown bottom that \napproximately has the same topology as the dental preparation region  above the margin line , and a shell that \nprovides the external 3D shape of a tooth (see Figure 2). Given the available ground  truth data of a crown \nbottom, our approach focuses on segmentation of the region supporting the crown bottom component from the \nrest of the die using deep learning, then extracting the margin line from the boundary of the segmented region. \nThe use of crow ns designed by dental technicians to map ground truth labels on dies is an apparent contribution \nof our work compared with [8], which used an S -Octree model to provide ground truth labeling on the dies. \nMoreover, only four test cases were qualitatively pre sented in [8] out of forty test cases and no quantitative \nstatistics were reported for results in that study.  \n \nFigure 2. Labeling ground truth die. The crown -bottom part of the crown was extracted from the crown as shown and its \nboundary -band inner curve  was used with the die scan to label the region above the margin line.  \nBecause dental preparations are characterized with variable geometries that differ according to the tooth \nposition and patient, existing methods still cannot meet the requirements for b oth efficiency and accuracy. \nHence, an accurate and automatic extraction of the margin line technique is still missing, and it is the main \nobjective of this manuscript to address this challenge. The methods are presented in the next section, followed \nby re sults, discussion, and conclusion.  \n2. Materials and Methods  \n2.1. Data preparation and labeling  \nIn this study we used 54 die cases of oral patient's dental preparation models including incisors from the \nupper and lower arches. The standard labels of the se lected incisors are 11, 21, 31, and 41. Out of all data, 41 \nsamples (almost 75%) were used for training and 13 samples (almost 25%) were used for testing. According to \ndental technicians, identifying margin lines on incisors is more difficult than molars o r premolars due to the \npossible absence of features. Therefore, this study focused on incisors only.  \nThe input data consists of 3D surface scans of dies along with their corresponding master arches and dental \ncrowns in STL mesh format. Crowns designed by dental professionals were used to extract the ground truth \nmargin lines. The ground truth margin lines were obtained by extracting the boundary from the crown bottom \n\n\n--- Page 4 ---\nPre-print  \n \nPage 4 of 15 \n by selecting edges which belong to single triangles. Margin line points were then obtained  from the vertices of \nthe boundary edges. The margin line points were defined as the vertices that lie on the surface of the crown \nbottom that intersects the die mesh. Next, the closest triangles to the margin line points were selected from the \ndie mesh, w hich represents margin line faces. The margin line faces split the die mesh into two regions with \ntwo distinct labels: the upper region represents the crown bottom that contains the margin line faces as its \nboundary (purple region in Figure 2) while the bo ttom region represents the rest of the die (Figure 2). As the \nmargin line triangular faces become smaller, their centroids provide a better approximation of the ground truth \nmargin lines.  \n \nFigure 3 . Overall inference framework based on a pre -trained deep  learning model to segment prepped teeth. The input die \nis registered and decimated to 10 k triangles. The die is next segmented using the pre -trained segmentation model into two \nregions. The segmentation results are refined using the graph -cut technique w hich applies boundary smoothing. The \nboundary triangles of the upper region (i.e. crown bottom) are extracted and their centers are used to interpolate a spline \nrepresenting the generated margin line.  \n2.2. Data preprocessing  \nData preprocessing involved ri gid registration and decimation. Labeled dies belonging to upper arches \nwere rotated 180\u00b0 such that they share the same orientation with dies belonging to lower arches (see Figure A1 \nin Appendix ). This was done through oriented bounding box registration wh ich uses a convex hull and then \napplies principal component analysis. After that, dies were decimated to reduce the number of triangles in the \nmesh to 10,000 triangles.  \nAugmentation was applied to increase the amount of data such that 20 samples were gene rated from each \ndie by random rotations in the ranges of [ -45\u00b0, 45\u00b0] around the x and y axes, [ -180\u00b0, 180\u00b0] around the z axis, and \nscaling in the range of [0.9, 1.1] in the x, y, and z dimensions. The selected ranges were chosen such that the \noriented boun ding box registration is not excessively disrupted. For instance, rotation of the samples around \nthe x and y axes was limited as opposed to the z axis. Finally, dies were normalized by subtracting the mean \nand dividing by standard deviation of their coordi nates. The final number of training data after augmentation \nwas 861 samples.  \n2.3. Segmentation deep learning network  \nThe state -of-the-art MeshSegNet architecture [20] was chosen as the segmentation network for supervised \ntraining which is based on pointNe t++ [23] and includes curvature features to improve teeth segmentation. \nSegmentation in this setting means to assign a label to every triangle on the die mesh to classify whether it \nbelongs to the region of the preparation which supports the crown bottom o r not. MeshSegNet works on various \nraw surface attributes as inputs including coordinates and normal vectors. It extracts multi -scale local \ncontextual features  hierarchically by integrating a sequence of graph -constrained learning modules, each fed \n\n\n--- Page 5 ---\nPre-print  \n \nPage 5 of 15 \n by a mu lti-layer perceptron (MLP) along its forward pass. Next, local -to-global geometric characteristics are \ncombined using a dense fusion technique to learn higher -level features for mesh cell labelling [20]. The output \nof the dense fusion block is passed to an other MLP before applying a 1D convolution to obtain class \nprobabilities. Figure 4 shows the network architecture [20]. This network was preferred to other semantic \nsegmentation models [19] due to its proven ability to distinguish between teeth and gingiva . We modified the \nnetwork by including 3 more input channels to account for curvature.  \nThe training data was split into 5 folds. Four folds were used during training and a single fold was used \nas a validation set. The validation fold was shifted which res ulted in 5 different trained models. An ensembled \napproach was used by combining results from the five different models. The model hyperparameters were \nselected as follows: the learning rate was 1 \u00d7 10-3, the train and validation batch sizes were 10, Adam was used \nas the optimizer, the patch size was 10000, and the number of used classes was 2. The number of input channels \nwas varied between 9 and 18 input channels to investigate the effect of different input channels on the model \nperformance. The input cha nnels entailed 9 channels for vertex cartesian coordinates, 3 channels for the \ncartesian coordinates of the barycenter of each triangle in the mesh, 3 channels for the normal vectors of the \ntriangles\u2019 centers, and 3 channels for the discrete mean curvature s of each triangle\u2019s vertices. The discrete mean \ncurvatures were calculated by selecting a characteristic radius for every mesh that equals the maximum edge \nlength of that mesh. All other model parameters were kept as default based on the architecture of M eshSegNet \n[20].  \n \n \n \nFigure 4 : Architecture of MeshSegNet with modified input and output channels. MLP: Multi -layer  \nperceptron; FTM: Feature -transformer module; GLM: graph constrained learning module with symmetric \naverage pooling; GMP: Global matrix poolin g; U: Upsampling; AS: Small scale adjacency matrix; AL: Large \nscale adjacency matrix.  \nThe training was carried out for 200 epochs/iterations for all considered models. The training loss was used \nto update the weights of the model whereas the validation los s was used to choose the best model. Four \nsegmentation models with different configurations were compared to evaluate the effect of different model \nparameters on the segmentation performance. Table 1 lists the different models with the main differences in the \nconfigurations.  \n2.4. Postprocessing and margin line extraction  \n\n\n--- Page 6 ---\nPre-print  \n \nPage 6 of 15 \n  Once the crown bottom region was predicted on the decimated die, geometric post processing for \nboundary optimization was applied using the graph -cut method to refine the prediction. This was done by \ncombining the graph cut method and the labelling probabilities to refine the segmentation results and enhance \nthe smoothening of the boundaries [2 4, 25]. After that, the die was re -oriented to the original position. To obtain \na margin line, the  region corresponding to the crown bottom on the preparation was separated from the rest of \nthe die and its boundary faces were selected to represent the margin line faces. The boundary face centers were \nobtained and used to find the closest points on the surface of the original die scan. A B -spline was then \ninterpolated through that set of 3D points with a trade -off between closeness and smoothness [33]. The \nscipy.interpolate.splprep method was used for B -spline fitting with a smoothness value of 0.005 (\ud835\udc41\u2212\u221a2\ud835\udc41), \nwhere N is the number of data points. Finally, the points on the surface of the die that are closest  to the \ninterpolated spline were located and chosen to represent the points of the margin line.  \n \nTable 1  \nModels\u2019 configurations used for differen t experiments  \nModel number  Inclusion of points \ncoordinates channels  Inclusion of curvature \nchannels  Count of cells after \ndecimation  \n1 Yes Yes 10,000  \n2 Yes No 10,000  \n3 Yes Yes 20,000  \n4 No Yes 10,000  \n \n2.5. Ensemble models  \n We used a cross -validation t echnique and a voting classifier to combine the results coming from the 5 -\nfolds of the trained MeshSegNet deep learning model. Two techniques were investigated for the voting \nclassifier: maximum probability and democracy. In maximum probability, the model that returned the \nmaximum probability for a label was selected to assign that label. In democracy, a voting strategy among the \nfive folds was considered to select the appropriate label of a triangle. Both maximum probability and democracy \ntechniques were c ompared with the five different folds of the best model selected from Table 1.  \n      \n2.6. Overall Framework of the proposed method  \n      \n The proposed framework entails two major components: 1) Training a deep learning segmentation \nmodel to segment dies or  prepped teeth; 2) Using the deep learning model to segment dies at inference stage \nthen extracting the margin line. In both steps, the input die data should be registered and decimated to only 10 \nk triangles. During the training step, the input dies were labeled as described in sections 2.1 (data preparation \nand labeling) and augmented as described in section 2.2 (data preprocessing). Figure 3 illustrates the inference \nstage where the input data consists of unlabeled dies. After registration, the dies are segmented using the pre -\ntrained model and then post -processed. The post -processing process involves refining the segmentation results \nusing the graph -cut method and then extracting the margin line as described in sec 2.4 (Postprocessing and \nmargin line ext raction).  \n2.7. Validation methods  \n2.7.1. Metrics  \n Three metrics were used to quantitatively evaluate the performance of the segmentation model. \nNamely, dice similarity coefficient (DSC), positive predictive value (PPV), and sensitivity (SEN) whose values  \nrange between 0 and 1 and are positively proportional to the segmentation performance. The sensitivity, also \ncalled recall or true positive rate, measures the portion of positive triangles in the ground truth that are also \nidentified as positive by the se gmentation being evaluated [2 6]. The positive predictive value or the precision \nis the ratio of relevant true positive predictions among the positive predictions. Dice similarity coefficient is \nalso called the overlap index or the F1 score, is the most use d metric in direct comparison between automatic \nand ground truth segmentations [2 6]. The DSC is a harmonic average  of precision and recall and is used to \nmeasure the accuracy of the test. The prediction metrics were calculated in Eqs. (1 -3) based on the pr edicted \n\n--- Page 7 ---\nPre-print  \n \nPage 7 of 15 \n labels of the mesh triangles being true positive (TP), false positive (FP), true negative (TN), and false negative \n(FN).  \n \n\ud835\udc37\ud835\udc46\ud835\udc36 =\ud835\udc391 \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 =\ud835\udc4e\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66 = 2\ud835\udc47\ud835\udc43\n2\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc43+\ud835\udc39\ud835\udc41                                (1) \n\ud835\udc46\ud835\udc38\ud835\udc41 =\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc56\ud835\udc61\ud835\udc66 =\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 =\ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52  \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52  \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52 = \ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc41  (2) \n\ud835\udc43\ud835\udc43\ud835\udc49 =\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc43                                                                     (3)  \n  \nFor more details, a qualitative illustration is provided in Figure 5. Two additional  metrics were also \nreported to evaluate the predicted margin line which are the maximum and mean distances  between the \npredicted and true margin lines. These distances were calculated based on the minimum spatial Euclidian \ndistance between the two-point  clouds of the predicted and ground truth margin lines. Finally, we reported the \noverall number of successful  test cases based on a threshold defined by the acceptable human error produced \nby a dental technician while manually creating margin lines. The value of the threshold was 200 \u03bcm. More \ndetails are presented in the Results and Discussion sections.    \n \nFigur e 5. Qualitative illustration of predicted labels; TP: true positive, FP: false positive, TN: true negative, and FN: false \nnegative.  \n2.7.2. Rating of preps and margin lines  \nThe quality of ground truth margin lines was rated by a collaborating dental techni cian. Due to the absence \nof standardized criteria for the evaluation of preparations, we depended on guidelines from published \nliterature [1, 27 -28] as well as the opinion of an expert dental lab technician to rate the quality of the preparation \n(Kerenor D ental Studio, Montreal, Canada). For the score, we considered the margin line (visibility, regularity, \nsmoothness, single or double margin line), the space for the crown thickness, the existence of sharp feature \nedges on the top of the prepared tooth, and the presence of undercuts. The rating was scored out of 4 such that \n4 is very good, 3 is good, 2 is acceptable, 1 unworkable. The rating procedure was iterative such that two dental \ntechnicians qualitatively evaluated the preparations against the guideline s available in the literature [27 -29]. \n \n2.7.3. Statistics  \n \nWe used the Shapiro -Wilk test to check normality of the data. This test showed that the values of the \nsegmentation metrics  reported in Table 2 and Table 3 are not normally distributed. Therefore, t he non -\nparametric Kruskal -Wallis test was used to investigate the significance among the values of the segmentation \nmetrics  reported in Table 2 and Table 3. Bonferroni\u2019s test was used for post hoc multiple comparisons. All \nstatistical analyses were perform ed using R software, and p values of less than 0.05 were considered as \nindicating statistical significance.  \n\n\n--- Page 8 ---\nPre-print  \n \nPage 8 of 15 \n 3. Results   \n \nThe quantitative results comparing the different models are presented in Table 2 and Table 3. While there \nwas no statistical significa nce among the ranks of four model configurations in Table 2 in terms of the \nsegmentation metrics, Model 1 scored the best in terms of the number of successfu lly predicted test cases. \nTherefore, the configurations of model 1 (i.e. inclusion of coordinates a nd curvature channels) were used to \nstudy the effect of using an ensemble technique which combined results from the five folds of model 1.  \n3.1. Quantitative and qualitative results of best model  \nAll predicted margin lines seal the prep aration  with zero dis tance from the preparation 3D surface. Table \n3 shows the results of the five folds and the combined models. The best overall model outcomes were for the \ncombined models with maximum probability, which reported successful margin line generation for 7 cases out \nof 13 test cases. Table 4 shows that t he average maximum distance between predicted and true margin line s \nwas 194 \u00b5m, the average mean distance was 70.7 \u00b5m, and the average standard deviation of the distance was \n45.6 \u00b5m (see Figure 6 ). Sample qualitati ve results in Figure 7 show minor differences between predicted and \ntrue margin lines.  \n \nFigure 6. Maximum, average, and variances in the distances of predicted versus true margin line for 1 3 test cases.  \n \n\n\n--- Page 9 ---\nPre-print  \n \nPage 9 of 15 \n Figure 7. Qualitative results showing minor diffe rences between predicted (red) and true margin lines (black) for test cases \nfrom positions 21 in (a) and 11 in (b).  \n3.2. Effect of decimation rate  \nComparing model 1 with model 3  in Table 2 , a decimation to 10 k triangles provided more successful cases \n(6 cases)  compared with 20 k  (4 cases) . This is because a patch size of 9980 cells was fixed for both models. \nTherefore, a mesh with 20 k cells lose s details about the margin line compared with a mesh with 10 k cells. In \naddition, the decimation algorithm favo red the creation of cell boundaries along feature lines for the meshes \ndecimated to 10 k cells.  \n3.3. Effect of input channels  \nThe effect of input channels was investigated by performing an ablation study of the curvature channels \n(3 channels) as well as t he vertex coordinate channels (9 channels). Comparing model 1 with model 2, adding \nthe curvature increased the number of successful cases (Table 1). Comparing model 1 and model 4, we can see \nthat removing the vertex positional channels decreased the number  of successful cases by 3. The inclusion of \ncurvature channels was essential as it reduced the fluctuations in the training and validation curves as shown \nin Figure 10 and it improved the margin line prediction results in terms of the number of successful test cases \n(Table 3).  \n3.4. Analysis of margin lines rating produced by dental technicians  \nThe average margin line score rated by dental technicians was 2.65 \u00b1 0.66 for the training data (41 samples) \nwhereas it was 2.5 \u00b1 0.65 for the test data (13 samples).  Our data analysis revealed a correlation between the \nmean distance between true and predicted margin lines for test data and the rating of the true margin lines \nproduced by dental technicians (Figure 8). The Spearman \u2019s rank correlation coefficient ( r-valu e) was found to \nbe -0.683, p-value = 0.010 (Table 4). In addition, we investigated the similarity of two margin lines, each \nproduced by a different dental technician for the same dental die. By considering a set of 3 different cases, \nFigure 9 shows that th e discrepancy between two margin lines produced for the same case could reach up to \n200 \u00b5m.  \n \nFigure 8. Correlation between the rating of true margin lines by dental technicians and the mean distance between true and \npredicted margin lines.  Spearman\u2019s rank  correlation coefficient = -0.683, p -value = 0.010.  \n \nTable 2  \nResults from models trained with different settings  \nModel  Dice Similarity \nCoefficient (DSC)  Sensitivity (SEN)  Positive Predicted \nValues  No. successful  \ncases  \n\n\n--- Page 10 ---\nPre-print  \n \nPage 10 of 15 \n (PPV)  [out of 13]  \n1 0.980\u00b10.006  0.973\u00b10.015  0.988\u00b10.012  6 \n2 0.974\u00b10.017  0.981\u00b10.012  0.968\u00b10.034  4 \n3 0.973\u00b10.020  0.968\u00b10.039  0.978\u00b10.022  4 \n4 0.955\u00b10.035  0.957\u00b10.048  0.957\u00b10.060  3 \n \n \nTable 3  \nResults of 5 -folds of model 1 and comparison with ensemble model combined using two strategies: maximum \nprobability and democracy  \nFold/metric  DSC  \n(Accuracy)  SEN  \n(Sensitivity)  PPV  \n(Specificity)  No. successful  \ncases  \n[out of 13]  \nFold 1  0.980\u00b10.006  0.973\u00b10.015  0.988\u00b10.012  6 \nFold 2  0.971 \u00b10.022  0.976\u00b10.016  0.967\u00b10.047  5 \nFold 3  0.978\u00b10.009  0.976\u00b10.017  0.981\u00b10.013  3 \nFold 4  0.974\u00b10.018  0.969\u00b10.014  0.979\u00b10.032  4 \nFold 5  0.973\u00b10.013  0.973\u00b10.019  0.973\u00b10.033  4 \nComb . max prob.  0.981 \u00b10.006  0.974 \u00b10.015  0.989 \u00b10.011  7 \nComb . democracy  0.981\u00b10.006  0.977 \u00b10.012  0.986 \u00b10.015  6 \n \nTable 4  \nRatings versus margin line pr ediction metrics for combined models with maximum probability.  \nPrep ID  Rating (out of 4)  Max distance predicted vs. \ntrue margins [\u00b5m]  Mean distance predicted \nvs. true margins [\u00b5m]  Standard deviation \npredicted vs. true margins \n[\u00b5m]  \n0862 -21 2.5 225 63 55 \n6138 -21 3 264 72 53 \n6158 -11 2 182 98 48 \n6158 -21 2 230 74 58 \n6223 -21 2 166 73 41 \n6225 -11 3 93 42 24 \n6225 -21 3 163 43 37 \n6227 -11 2 172 82 50 \n6227 -21 2 219 84 41 \n6230 -21 3 207 74 42 \n6231 -21 2 246 95 54 \n6242 -11 2 185 61 47 \n6260 -11 4 171 57 43 \nAvera ge 2.5 194 70.7 45.6 \n \n \n \nFigure 9. (a) Maximum and average distances between two margin lines produced by different technicians in blue and red, \nrespectively. The average maximum distance is 164 \u00b5m and the average mean distance is 65.2 \u00b5m. (b) Qualitative  \ncomparison between two margin lines produced by two different technicians for the same case.  \n\n\n--- Page 11 ---\nPre-print  \n \nPage 11 of 15 \n The inclusion of coordinates and curvature channels was essential to improve the accuracy of the model and \nthe quality of margin line prediction results (Table 3) . Because the curvature features were reported to be \nessential for margin lines, curvature features were added as input channels to train the neural network. Figure \n10 shows that curvature channels reduced the fluctuations in the training and validation cu rves and reduced \nthe validation loss improving the accuracy of the trained AI model.  \n \nFigure 10. Comparison between training and validation loss curves for the segmentation model trained with (18 channels) \nand without (15 channels) curvature channels . \n3.5. Analysis of unsuccessful test cases  \nIn this section, we discuss the features of unsuccessful test cases. Figure 11 shows the predicted segmentation \nof crown bottoms of 4 test cases along with the ground truth (green) and predicted (red) margin lines. The \nfigure shows that the predicted margin line for case (a) is slightly above the correct position. This case possesses \na pump at the margin line location which confuses the algorithm. A similar situation applies for case (b). For \ncase (c), the figure shows  that the margin line is slightly under the correct position on the front side and slightly \nabove the correct position on the back side  due to the presence of pumps on both sides.  In case (d), the algorithm \npredicted the margin line below the correct posit ion due to an error from the segmentation algorithm. The deep \nlearning model was confused during die segmentation due to the presence of multiple nearby features with \nhigh curvature.  \n \nFigure 11. Predicted segmentation of crown bottoms of 4 test cases alo ng with the ground truth (green) and predicted (red) \nmargin lines.  \n4. Discussion  \nAdvances in digital dentistry and artificial intelligence have increased the demands for automatic digital \ndesign and manufacturing. Among recent research advances in the fiel d is a robot -assisted tooth preparation \nbased on augmented reality [29] and automatic crown generation using deep learning [14]. Hence, the need for \n\n\n--- Page 12 ---\nPre-print  \n \nPage 12 of 15 \n automatic and accurate margin line extraction techniques is crucial. The use of AI -assisted margin line \nextraction tools promises a high potential to solve the margin line generation problem [29]. However, it is \ndifficult to generalize about the level of accuracy of AI for the detection of tooth preparation margins due to the \nsmall number of AI -based systems sp ecified for margin line extraction (just one study to date). As a result, more \nresearch is needed in this area [29]. While it is essential for a margin line to be the outermost region, this does \nnot necessarily always mean the region with the highest curva ture. This makes the process of automatically and \naccurately extracting an accurate margin challenging.  \nThe MeshSegNet architecture was chosen from a wide range of networks available in the literature for \npoint cloud segmentation [30] because it proved hi gh accuracy on segmentation of dental data and \nidentification of dental features. Although the architecture of MeshSegNet was used, the novelty of this work \nlies in re -training the network with different input channels to detect margin lines on the surface s of prepared \nteeth. Our application entails segmenting the tooth itself into two regions then extracting a margin line through \nspline fitting, rather than segmenting a particular tooth from a scanned jaw. Since curvature features were \nreported to be essen tial for margin lines [31], curvature features were added as input channels to train the neural \nnetwork. Our work also reveals that using an ensemble technique on MeshSegNet trained models helps increase \nthe accuracy of the predicted margin lines. In addit ion, we propose for the first time a novel technique to \ngenerate accurate ground truth labels for margin lines based on actual physical crown designs produced by \ndental professionals. We also presented, for the first time, a unique analysis to correlate th e quality of \npreparations produced by dentists with the quality of the margin lines predicted by the proposed framework.  \nThe boundary faces of the resulting segmented mesh (in particular the segmented crown bottom mesh) was \nused to create the spline repres enting the predicted margin line. A spline was approximated to best fit the \ncenters of the boundary faces of the segmented mesh to predict the margin line. The Euclidean distance metric \nwas used to estimate the total error between the ground truth margin l ines and the predicted splines. In order \nto do that, margin lines were discretized to 5,000 points each on the splines. After that the Euclidean distance \nbetween every two closest points on the predicted and ground truth margin lines was calculated. Table 4 in the \nshows the maximum of that Euclidean distance for each test case and also shows the average maximum distance \nfor all the test cases to be 194 \u00b5m. Out of 13 unseen test cases, the smallest maximum distance was 93 \u00b5m and \nthe greatest was 264 \u00b5m.  \nThe clinically accepted accuracy for the margin line region is 100 \u00b5m which corresponds to the accuracy of \nan intraoral scanner (IOS) [32]. On the other hand, we found that discrepancies in distances between two real \nmargin lines produced by two different tech nicians for the same case could reach up to 200 \u00b5m (Figure 9). The \nbest results obtained with our combined model with maximum probability showed an average maximum \ndistance of 194 \u00b5m between predicted and true margin lines, which is considered very close t o the human error \nreported in Figure 9. The 4th column of Table 4 considers the mean Euclidean distance between the ground truth \nand predicted margin lines and reports it for every test case. The average of this mean Euclidean distance was \nfound to be 70.7  \u00b5m. The rep orted results do not violate the distance threshold of 200 \u03bcm representing human \nerror. These results were achieved with the  combined model with maximum probability , which also showed \nan average standard deviation of 45.6 \u00b5m between the predicted and true margin lines. This result is within the \nrange of accepted accuracy and was achieved with a test set that constituted about 25% of the total dataset. \nCompared with [8], our proposed model achieves a higher accuracy (DSC = 0.981\u00b10.006) with less data. \nUnfort unately, there are no other studies in literature dealing with margin line detection on tooth preparations \nusing AI [29]. In addition, it is illogical to compare our results with the only available study in the literature [8], \nwhich claimed up to 97% accur acy, because the data are not the same. In fact, dental data is highly complex and \nvariable such that the presence of a couple of difficult cases could cause substantial changes in the accuracy \nscore.     \nGenerally, due to the high localization of the margi n line as well as the highly required accuracy and \nprecision, small differences in the segmentation metric results lead to large variances in the relative distances \nbetween predicted and true margin lines. Therefore, the sensitivity metric is extremely imp ortant in that regard. \nIn fact, with a higher sensitivity score, a model will more likely predict a margin line on or below the true \nmargin line which is more practical than a prediction above the true margin line. In other words, it is extremely \nimportant  to minimize the false negatives (see Figure 5). Combining the 5 folds with maximum probability led \nto the maximum number of successful test cases (Table 3). In 7 out of 13 test cases, the predicted margin lines \nby the ensemble model with maximum probabili ty had a maximum distance smaller than or equal to 200 \u03bcm.  \n\n--- Page 13 ---\nPre-print  \n \nPage 13 of 15 \n The average rating of true margin line was reported by the technicians to be 2.5 out of 4. This indicates the \npresence of a variant and problematic margin line in some cases and highlights the impo rtance of automatic \nand consistent  margin line extraction. Since the absolute r -value of the correlation was greater than 0.5, a \nrelationship exists between  the ratings of dental technicians for the true margin lines and the model predictions \nof the mean d istance between the predicted and true margin lines. A negative r-value of -0.683 indicates that as \nthe rating of tooth preparation  increases, the difference between the predicted margin and true margin lines \ndecreases. This also indicates that our model n aturally produces better margin lines as the quality of dental \npreparation increases.  \nThe automatic generation of margin lines could lead to automated teeth preparation for crown treatment \nwith help of simulated reality and digital cameras and even replac e the freehand method of creating a \npreparation [17]. Although the accuracy achieved by the proposed model is acceptable, there is still room for \nimprovement. It is expected that increasing the dataset size will increase the prediction accuracy.  \nThe size of our dataset was limited by the availability of data. We attempted to mitigate this issue by: (1) \nusing various well -known augmentation techniques to extensively increase the number of our training dataset \n(each sample was augmented 20 times); (2) focusi ng our study on a particular type of teeth to reduce the \nvariability given the limited amount of data (only 4 positions out of 28 positions were considered, which are 11, \n21, 31, and 41); (3) registering the data by rotating 180\u00b0 the labeled dies belonging  to upper arches such that \nthey share the same orientation with dies belonging to lower arches further reduced the 4 positions to only two \npositions that are also symmetric. For example: positions 11, 21, 13, 14 would be more different than 11, 21 ,31, \n41 (which are the positions used in our experiments).  \nAlthough the proposed framework was trained and tested on incisor teeth, it can be considered a novel \nfirst step towards a fully automated and accurate margin line detection and could be easily extended t o other \ntypes of teeth . Obviously, increasing the dataset increases the deep learning model accuracy. Nevertheless, this \nwork contributes a novel end -to-end framework for margin line extraction on prepared teeth more than a ready -\nto-use pretrained model. T he next step is to scale -up the model training on a larger dataset for all teeth positions \nand deploy a generic model online for external users. We aim to provide software to help dental technicians \nproduce better margin lines during the crown generation p rocess. The software interface will also be simple \nenough to be easily used and manipulated by interested dentists, dental students, and dental technologists.  \nFuture development will also focus on enhancing the preprocessing steps to improve the triangula tion \nresolution of tooth meshes, particularly near the margin line. Adaptive mesh refinement will be introduced \nbased on curvatures such as regions of high curvature will be more decimated. Although this enhancement \nincreases the number of points available  for more accurate spline prediction of the ground truth margin, it also \nresults in greater preprocessing time and memory usage. As such, a balance must be achieved between accuracy \nand computational efficiency . Another future direction is to improve the p ostprocessing pipeline to ensure the \nextraction of a margin line that avoids undercuts.  \n5. Conclusion  \nThis paper  presented a data -efficient deep learning framework for margin line extraction from dental \npreparations. The successful identification of a pro per margin line is a key factor in the success of the dental \ncrown design. The proposed end-to-end framework extracted margin lines with acceptable relative distances \nfrom the true margin lines designed by dental technicians for incisor teeth. The average maximum distance \nobtained with an ensemble model combined with maximum probability was 194 \u03bcm, which was close to the \nhuman measured error of 200 \u03bcm. It was also demonstrated that the better the quality of the preparation, the \nbetter the prediction of the margin line by the proposed model.  \nWe made the training and test datasets available to enable the community to compare other models with \nthe model used in this work for the same dataset2. Among the limitations of our study are the small number of \ndata use d in our experimental set up and the restriction of the teeth type to the first incisor. The future directions \ninvolve scaling up the dataset with more tooth types including other canines, incisors, premolars, and molars \nfrom both the upper and lower jaws and increasing the total number of training and test data. Future \n                                                           \n2 https://github.com/intellident -ai/teethPreparationData/tree/ma in \n\n--- Page 14 ---\nPre-print  \n \nPage 14 of 15 \n development will also entail improving the preprocessing procedure to increase the triangulation resolution of \nthe teeth meshes near the margin line region and improving the postprocessing b y smart correction of \nsegmentation results to extract more accurate margin lines.  \n \nAcknowledgments:  Author Francois Guibault acknowledge s the funding received  by the Natural Science and \nEngineering Research Council of Canada [Ref: ALLRP 549122 -2019], Inst itut de valorisation des donn\u00e9es \n(IVADO) [Ref: PostDoc -2020a -5943530233], and MEDTEQ  [19-D Volum\u00e9trie dentaire 2 ]. Author Ammar \nAlsheghri acknowledges the funding received from KFUPM [Ref: EC241009 ]. We thank the Canadian Digital \nAlliance for providing the  computational resources used in this work. The authors acknowledge the help and \nsupport from JACOBB and Comet Technologies Inc. (Formally Object Research Systems Inc). This work has not \nbeen submitted for publication or presentation elsewhere.  Author Amma r Alsheghri acknowledges the \nguidance of his mentor at KFUPM, Dr. Muhammad Hawwa.  \nAppendix : \n \nFigure A1. Dies sharing similar orientations as a preprocessing step before training.  \n \nReferences  \n1. Podhorsky, A.; Rehmann, P.; W\u00f6stmann, B. Tooth preparation for f ull-coverage restorations \u2014a literature review . J Clinical \noral investigations , 2015. 19(5): p. 959 -968. \n2. Ramesh, G.; Nayar, S.; Chandrakala, S. Principles of Tooth Preparation -Review Article . J Indian Journal of Forensic Medicine \nToxicology , 2020. 14(4). \n3. Yu, N., Dai, H.W., Tan, F.B., Song, J.L., Ma, C.Y. and Tong, X.L. Effect of different tooth preparation designs on the marginal  \nand internal fit discrepancies of cobalt -chromium crowns produced by computer -aided designing and selective laser \nmelting processe s. The Journal of Advanced Prosthodontics , 2021. 13(5): p. 333.  \n4. Seymour, K., D. Samarawickrama, and E. Lynch, Metal ceramic crowns --a review of tooth preparation . The European journal \nof prosthodontics restorative dentistry , 1999. 7(2): p. 79 -84. \n5. Abdullah,  A.O., Muhammed, F.K., Zheng, B. and Liu, Y. An overview of computer aided design/computer aided \nmanufacturing (CAD/CAM) in restorative dentistry. Journal of Dental Materials Techniques , 2018. 7(1): p. 1 -10. \n6. Yu, H., Chen, Y.H., Cheng, H. and Sawase, T. Finish-line designs for ceramic crowns: a systematic review and meta -analysis . \nJ The Journal of Prosthetic Dentistry , 2019. 122(1): p. 22 -30. \n7. Reich, S., A. Petschelt, and U. Lohbauer, The effect of finish line preparation and layer thickness on the failure lo ad and \nfractography of ZrO2 copings. J The Journal of prosthetic dentistry , 2008. 99(5): p. 369 -376. \n8. Zhang, B., Dai, N., Tian, S., Yuan, F. and Yu, Q. The extraction method of tooth preparation margin line based on S\u2010Octree \nCNN . International Journal for N umerical Methods in Biomedical Engineering , 2019. 35(10): p. 3241.  \n9. Ni, H., Lin, X., Ning, X. and Zhang, J, Edge detection and feature line tracing in 3D -point clouds by analyzing geometric \nproperties of neighborhoods. Remote Sensing , 2016. 8(9): p. 2072 -4292. \n10. ZHANG, C., Extraction of Dental Biological Feature Line Based on Heuristic Search Strategy . China Mechanical Engineering , \n2012. 23(13): p. 1567.  \n11. Li, X., Wang, X. and Chen, M. Accurate extraction of outermost biological characteristic curves in tooth pr eparations with \nfuzzy regions. J Computers in Biology Medicine , 2018. 103: p. 208 -219. \n12. Shan, T., Tay, F.R. and Gu, L.  Applications of artificial intelligence in dentistry: A comprehensive review. Journal of Esthetic \nand Restorative Dentistry  34.1 (2022): 2 59-280. \n13. Alshadidi, A.A.F., Alshahrani, A.A., Aldosari, L.I.N., Chaturvedi, S., Saini, R.S., Hassan, S.A.B., Cicci\u00f9, M. and Minervini,  \nG. Investigation on the application of artificial intelligence in prosthodontics. Applied Sciences  13.8 (2023): 5004.  \n\n\n--- Page 15 ---\nPre-print  \n \nPage 15 of 15 \n 14. Less ard, O., Guibault, F., Keren, J. and Cheriet, F. Dental Restoration using a Multi -Resolution Deep Learning Approach. In \nProceedings of IEEE 19th International Symposium on Biomedical Imaging (ISBI), Kolkata, India, 28 -31 March 2022; IEEE: \nNew York City, Un ited States, 26 April 2022.  \n15. Hosseinimanesh, G., Ghadiri, F., Alsheghri, A., Zhang, Y., Keren, J., Cheriet, F. and Guibault, F. Improving the quality of \ndental crown using a transformer -based method. In Medical Imaging 2023: Physics of Medical Imaging (Vol.  12463, pp. 802 -\n809). SPIE.  \n16. Pich\u00e9, N., et al,. (2023). U.S. Patent Application No. 18/017,809, filed 2023 Sep 7.  \n17. Tian, S., Wang, M., Ma, H., Huang, P., Dai, N., Sun, Y. and Meng, J. Efficient tooth gingival margin line reconstruction via \nadversarial learni ng. J Biomedical Signal Processing Control , 2022. 78: p. 103954.  \n18. Jiang, X., Xu, B., Wei, M., Wu, K., Yang, S., Qian, L., Liu, N. and Peng, Q. C2F -3DToothSeg: Coarse -to-fine 3D tooth \nsegmentation via intuitive single clicks. J Computers Graphics , 2022. 102: p. 601 -609. \n19. Alsheghri, A., Ghadiri, F., Zhang, Y., Lessard, O., Keren, J., Cheriet, F. and Guibault, F. Semi -supervised segmentation of \ntooth from 3D scanned dental arches. in Medical Imaging 2022: Image Processing (Vol. 12032, pp. 766 -771). SPIE.  \n20. Lian, C .; Wang, L.; Wu, T.H.; Wang, F.; Yap, P.T.; Ko, C.C.; Shen, D. Deep multi -scale mesh feature learning for automated \nlabeling of raw dental surfaces from 3D intraoral scanners. IEEE transactions on medical imaging  2020. 39(7) , p. 2440 -2450.  \n21. Im, J., Kim, J.Y., Yu, H.S., Lee, K.J., Choi, S.H., Kim, J.H., Ahn, H.K. and Cha, J.Y. \"Accuracy and efficiency of automatic \ntooth segmentation in digital dental models using deep learning .\" Scientific reports  12.1 (2022): 9429.  \n22. Tabatabaian, F., Vora, S. R ., Mirabbasi, S. Applications, functions, and accuracy of artificial intelligence in restorative \ndentistry: A literature review. Journal of Esthetic and Restorative Dentistry , 2023, 35(6), 842 -859. \n23. Qi, C.R., Su, H., Mo, K. and Guibas, L.J. Pointnet: Deep l earning on point sets for 3d classification and segmentation. in \nProceedings of the IEEE conference on computer vision and pattern recognition. 2017.  \n24. Zhang, R., Li, G., Wunderlich, T. and Wang, L. A survey on deep learning -based precise boundary recovery o f semantic \nsegmentation for images and point clouds . International Journal of Applied Earth Observation Geoinformation , 2021. 102: p. \n102411.  \n25. Rodrigues, R.S., J.F. Morgado, and A.J. Gomes. Part\u2010based mesh segmentation: a survey . in Computer Graphics Forum . 2018. \nWiley Online Library.  \n26. Taha, A.A.; Hanbury, A. Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool . J BMC medical \nimaging , 2015. 15(1): p. 1 -28. \n27. Goodac re, C.J., Campagni, W.V. and Aquilino, S.A. Aquilino. \"Tooth preparations for complete crowns: an art form based \non scientific principles.\" The Journal of prosthetic dentistry  85.4 (2001): 363 -376. \n28. Blair, F.M., Wassell, R.W. and Steele, J.G. \"Crowns and ot her extra -coronal restorations: Preparations for full veneer crowns.\" \nBritish dental journal 192.10 (2002): 561 -571. \n29. Jiang, J., Guo, Y., Huang, Z., Zhang, Y., Wu, D., Liu, Y. Adjacent surface trajectory planning of robot -assisted tooth \npreparation based on  augmented reality. Engineering Science Technology, an International Journal, 2022. 27: p. 101001.  \n30. Xie, Y., Tian, J., Zhu, X.X. Linking points with labels in 3D: A review of point cloud semantic segmentation . J IEEE Geoscience \nRemote Sensing Magazine, 2020 . 8(4): p. 38 -59. \n31. Kuralt, M., Cmok Ku\u010di\u010d, A., Ga\u0161per\u0161i\u010d, R., Gro\u0161elj, J., Knez, M., Fidler, A. Gingival shape analysis using surface curvature \nestimation of the intraoral scans. BMC Oral Health, 2022. 22(1): p. 1 -11. \n32. Son, K.; Lee, K. B. Effect of finish line locations of to oth preparation on the accuracy of intraoral scanners. J Int. J. Comput. \nDent , 2021. 24: p. 29 -40. \n33. Dierckx, P. Curve and surface fitting with splines, Monographs on Numerical Analysis , 1993, Oxford University Press.",
  "project_dir": "artifacts/projects/enhanced_cs.LG_2507.22859v1_Mesh_based_segmentation_for_automated_margin_line_",
  "communication_dir": "artifacts/projects/enhanced_cs.LG_2507.22859v1_Mesh_based_segmentation_for_automated_margin_line_/.agent_comm",
  "assigned_at": "2025-07-31T22:00:29.851353",
  "status": "assigned"
}